{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 1. Import all required Modules and custom build Classes into the Jupyter notebook\n",
    "Import all required modules for the project.<br />\n",
    "Import all custom modules for the project and instantiate classes using the config file.<br />\n",
    "Custom modules can now be called directly in the notebook.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from sys import getsizeof as dictsize\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pymongo\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "import urllib.request\n",
    "import missingno as msno\n",
    "import zipfile \n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "try:\n",
    "    import createNewData.data.config as in_config\n",
    "    from createNewData.pypackages.Azure import Azure\n",
    "    from createNewData.pypackages.urlHandler import UrlHandler\n",
    "    AzurePackage = Azure(in_config)\n",
    "    Url = UrlHandler(in_config)\n",
    "    \n",
    "    \n",
    "except ImportError as e:\n",
    "    print(in_config.FailedImport)\n",
    "    print(e)"
   ]
  },
  {
   "source": [
    "# 2. Upload to SQL Database\n",
    "Download the entire GTFS data and upload to the Azure SQL Database.\n",
    "Required fields:\n",
    "* \"UploadToSQL\": The class name for the Azure constructor for uploading tables to the database.\n",
    "* GTFSDF:  The dataframes to be written to the database.\n",
    "* tablename: The name of the new table to be created.\n",
    "* teamConnQuote: The connection string for the team SQL Database obtained from the config file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GTFS = requests.get(in_config.GTFSURL, stream=True)\n",
    "GTFSRaw = zipfile.ZipFile(io.BytesIO(GTFS.content))\n",
    "\n",
    "for each in GTFSRaw.namelist():\n",
    "    GTFSDF = pd.read_csv(GTFSRaw.open(each), low_memory=False)\n",
    "    tableName = each.replace(\".txt\",\"\")\n",
    "    AzurePackage(\"UploadToSQL\",\n",
    "                  GTFSDF,\n",
    "                  tableName,\n",
    "                  in_config.teamConnQuote)"
   ]
  },
  {
   "source": [
    "# 3. Create Request Batches\n",
    "1. Collect all items in the 'stops' schema of the shared team Database.\n",
    "2. Reduce Dataframe by removing dupicate coordinates.\n",
    "3. Pop() coordinates based on batch sizes no greater than 1024 bytes.\n",
    "4. Add each coordinate to a list of batches to send the the Open-elevations API\n",
    "\n",
    "### Attributes\n",
    "* Azure class imported with call functionality (in)\n",
    "* Config File (in)\n",
    "* listOfBatches (out)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "listOfBatches = []\n",
    "batches = {\"locations\" : []}\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\"ingestRawData\\gtfs\\shapes.txt\")\n",
    "    shapesDF = df[[\"shape_id\", \"shape_pt_lat\",\"shape_pt_lon\"]]\n",
    "    shapesRequest = shapesDF.drop_duplicates(subset=None, \n",
    "                                        keep='first', \n",
    "                                        inplace=False)\n",
    "    shapesCoordinates = Url(\"generateLocationRequest\", shapesRequest)\n",
    "    for key, value in shapesCoordinates.items():\n",
    "        if key == \"locations\":\n",
    "            locations = value\n",
    "    while len(locations) != 0:\n",
    "        for each in locations:\n",
    "            if dictsize(batches[\"locations\"]) +\\\n",
    "                                dictsize(each) +\\\n",
    "                                dictsize(batches) < 9700:\n",
    "                location = locations.pop(0)\n",
    "                batches[\"locations\"].append(location)\n",
    "            else:\n",
    "                location = locations.pop(0)\n",
    "                batches[\"locations\"].append(location)\n",
    "                listOfBatches.append(batches)\n",
    "                batches = {\"locations\" : []}\n",
    "        for each in listOfBatches:\n",
    "            if dictsize(each) > 10000:\n",
    "                raise Exception(in_config.RequestToBig)\n",
    "            else:\n",
    "                pass\n",
    "    else:\n",
    "        print(\"All values added to the list of requests.\")\n",
    "except pd.io.sql.DatabaseError as e:\n",
    "    print(in_config.NoSQLShema)\n",
    "\n",
    "except urllib.request.HTTPError as e:\n",
    "    if e.code == \"403\":\n",
    "        print(in_config.SQLConnectionFail)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(in_config.UNKMGO)\n",
    "    print(e)\n"
   ]
  },
  {
   "source": [
    "# 4. Send Request Batches\n",
    "1. Ssend all \n",
    "Attributes\n",
    "Azure class imported with call functionality (in)\n",
    "Config File (in)\n",
    "listOfBatches (out)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfObjects = []\n",
    "listOfElevations = []\n",
    "ListOfDicts = []\n",
    "\n",
    "Iteration = 0\n",
    "try:\n",
    "    for each in listOfBatches:\n",
    "            attempts = 0\n",
    "            while attempts < 5:\n",
    "                try:\n",
    "                    ListOfDicts.append(Url(\"mineElevationData\",each))\n",
    "                    break\n",
    "                except urllib.error.HTTPError:\n",
    "                    attempts = attempts+1\n",
    "            Iteration = Iteration + 1\n",
    "            print(Iteration)\n",
    "            time.sleep(2)\n",
    "    for each in ListOfDicts:\n",
    "        for key, value in each.items():\n",
    "            if type(value) is list:\n",
    "                listOfObjects.append(value)\n",
    "\n",
    "    for each in listOfObjects:\n",
    "        for elevation in each:\n",
    "            listOfElevations.append(elevation)\n",
    "        \n",
    "\n",
    "    df = pd.DataFrame(listOfElevations)\n",
    "    dfTrimmed = df.drop_duplicates()\n",
    "    sumElevation = dfTrimmed[\"elevation\"].sum()\n",
    "    if type(sumElevation) in [np.int64,int]:\n",
    "        print(\"Elevations collected correctly\")\n",
    "        print(dfTrimmed.head())\n",
    "    else: \n",
    "        raise Exception(\"Failed to collect all elevations, please try again.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Column {e} cannot be found in the dataframe.\")\n",
    "except NameError as e:\n",
    "    print(f\"The Datatable {e} cannot be found.\")\n",
    "except Exception as e:\n",
    "    print(in_config.UNKMGO)\n",
    "    print(type(e))\n",
    "    print(e)\n",
    "\n",
    "    "
   ]
  },
  {
   "source": [
    "# 5. Save Trimmed Elevation Data Team SQL Database\n",
    "This reads data from the raw Json files in the MongoDB database and imports them to a dataframe that contains only unique values. Removing any rows where any full row duplicates exist.\n",
    "This saves the resulting SQL schemata to the Database used by the R-Shiny app. (Production)\n",
    "This will overwrite any existing data in the SQL schemata that already exists."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SqlDataCursor = AzurePackage(\"UploadToSQL\",\n",
    "                              dfTrimmed,\n",
    "                              \"elevations\",\n",
    "                              in_config.teamConnQuote)"
   ]
  },
  {
   "source": [
    "# 6. Collect real time data and upload to Mongo\n",
    "Collect the real time data as a json file and overwrites the collection in the NoSQL database.\n",
    "* URL used https://gtfsr.transportforireland.ie\n",
    "* Azure cosmos was initially used but this turned out to be insufficient when storing larger data sets that occure earlier in the day. A Casandra database would be better suited to the RTI data as it can accept uploads greater than 2MB."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = in_config.url2\n",
    "headers = in_config.RTIheaders\n",
    "response = Url(\"callURL\", url, {}, headers)\n",
    "JsonData = response.read().decode('utf8').replace(\"'\", '\"')\n",
    "RTIgtfs = json.loads(JsonData)\n",
    "try:\n",
    "    AzurePackage(\"UploadToMongo\",\"RTIgtfs\",RTIgtfs)\n",
    "except pymongo.errors.WriteError as e:\n",
    "    print(\"An error occured while attempting to write the GTFS data to Mongo Database.\")\n",
    "    print(type(e))\n",
    "except HTTPError as e:\n",
    "    print(\"An error occured while attempting to connect to the CosmosDB Database.\")\n",
    "    print(type(e))\n"
   ]
  },
  {
   "source": [
    "# 7. Connect the shape and elevation schema together by joining their longtitude and latatude values\n",
    "This will either be a method in the Rshiny app to collect or we simply create a new database from this data but this seems a bit verbose."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conn = AzurePackage(\"AzureDBConn\", in_config.teamConnQuote)\n",
    "shapeListDF = []\n",
    "\n",
    "SQLTrips = \"\"\"SELECT distances.route_id, \n",
    "                     distances.service_id, \n",
    "                     distances.quasi_block,\n",
    "                     distances.trip_id,\n",
    "                     trips.shape_id\n",
    "               FROM distances\n",
    "               JOIN trips\n",
    "               ON distances.trip_id = trips.trip_id\n",
    "               WHERE distances.route_id = '60-1-d12-1'\n",
    "               AND distances.service_id = 'y1003'\n",
    "               AND distances.quasi_block = '1.0'\n",
    "               GROUP BY distances.route_id, distances.service_id, distances.quasi_block, distances.trip_id, trips.shape_id\n",
    "               \"\"\"\n",
    "tripsIds = pd.read_sql(SQLTrips, conn)\n",
    "for each in tripsIds[\"shape_id\"]:\n",
    "    SQLShapes = f\"\"\"SELECT shapes.shape_id, shapes.shape_pt_sequence, elevations.elevation, shapes.shape_dist_traveled\n",
    "                    FROM shapes\n",
    "                    LEFT JOIN elevations ON\n",
    "                    shapes.shape_pt_lon = elevations.longitude \n",
    "                    AND shapes.shape_pt_lat = elevations.latitude\n",
    "                    WHERE shapes.shape_id = '{each}'\n",
    "                    ORDER BY shapes.shape_id, shapes.shape_pt_sequence\n",
    "                    \"\"\"\n",
    "    shapeIDs = pd.read_sql(SQLShapes, conn)\n",
    "    shapeListDF.append(shapeIDs)\n",
    "fullShapesDF = pd.concat(shapeListDF).reset_index(drop=True)\n",
    "currentValue = 0\n",
    "lastValue = 0\n",
    "fullShapesDF[\"full_Distance\"] = np.nan\n",
    "\n",
    "for index, each in fullShapesDF.iterrows():\n",
    "    if each[3] == 0:\n",
    "        lastValue = 0\n",
    "        fullShapesDF[\"full_Distance\"][index] = currentValue\n",
    "    else:\n",
    "        currentValue = currentValue + (each[3] - lastValue)\n",
    "        lastValue = each[3]\n",
    "        fullShapesDF[\"full_Distance\"][index] = currentValue\n",
    "plot = fullShapesDF.plot(y = \"elevation\",x = \"full_Distance\",title='Elevation over Distance')\n",
    "plot.set_xlabel(\"Distance (M)\")\n",
    "plot.set_ylabel(\"Elevation (M)\")"
   ]
  },
  {
   "source": [
    "# 8. For Single Use SQL Queries\n",
    "Send SQL queries to team shared SQL database.\n",
    "This script is for development purposes only.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "conn = AzurePackage(\"AzureDBConn\", in_config.teamConnQuote)\n",
    "SQLString = \"\"\"Select * FROM [stops]\"\"\"\n",
    "stopdf = pd.read_sql(SQLString, conn)\n",
    "print(stopdf)\n",
    "conn.close()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# 9. Test For Missing Elevation Data\n",
    "Display all missing data in elevations when joined to the distances and stops schemata.\n",
    "White lines in the elevation column indicates missing data in the elevation column."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = AzurePackage(\"AzureDBConn\", in_config.teamConnQuote)\n",
    "SQLTest = \"\"\"SELECT distances.route_id, \n",
    "                     distances.service_id, \n",
    "                     distances.quasi_block,\n",
    "                     distances.time_axis,\n",
    "                     stopEelevations.elevation\n",
    "               FROM distances\n",
    "               LEFT JOIN stops ON distances.stop = stops.stop_id\n",
    "               LEFT JOIN stopEelevations ON stops.stop_lat = stopEelevations.latitude AND\n",
    "               stops.stop_lon = stopEelevations.longitude\n",
    "               \"\"\"\n",
    "merged = pd.read_sql(SQLTest, conn)\n",
    "msno.matrix(merged[[\"route_id\",\"elevation\"]], figsize=(10,5), fontsize=20)\n",
    "gray_patch = mpatches.Patch(color='gray', label='Data present')\n",
    "white_patch = mpatches.Patch(color='white', label='Data absent ')\n",
    "plt.legend(handles=[gray_patch, white_patch],loc='upper left', bbox_to_anchor=(1, 0))\n",
    "plt.show()\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "source": [
    "# 10. Display Elevations over Distance\n",
    "This plot displays the rise and fall of the ladscape as the bus travels through the routes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "conn = AzurePackage(\"AzureDBConn\", in_config.teamConnQuote)\n",
    "SQLTest = \"\"\"SELECT distances.route_id, \n",
    "                     distances.service_id, \n",
    "                     distances.quasi_block,\n",
    "                     distances.time_axis,\n",
    "                     stopEelevations.elevation\n",
    "               FROM distances\n",
    "               LEFT JOIN stops ON distances.stop = stops.stop_id\n",
    "               LEFT JOIN stopEelevations ON stops.stop_lat = stopEelevations.latitude AND\n",
    "               stops.stop_lon = stopEelevations.longitude\n",
    "               WHERE distances.route_id = '60-1-d12-1'\n",
    "               AND distances.service_id = 'y1003'\n",
    "               AND distances.quasi_block = '1.0'\n",
    "               \"\"\"\n",
    "merged = pd.read_sql(SQLTest, conn)\n",
    "conn.close()\n",
    "\n",
    "elevationArray = merged[[\"elevation\"]].values.flatten()\n",
    "timeArray = merged[[\"time_axis\"]].values.flatten()\n",
    "\n",
    "\n",
    "d = merged[\"time_axis\"].dt.to_pydatetime()\n",
    "plt.plot_date(timeArray,elevationArray, \"-\",color='green')\n",
    "plt.fill_between(timeArray, elevationArray, where=elevationArray>0, facecolor='green')\n",
    "plt.ylim([0,100])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Bus Route Time\")\n",
    "plt.ylabel(\"Elevations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}