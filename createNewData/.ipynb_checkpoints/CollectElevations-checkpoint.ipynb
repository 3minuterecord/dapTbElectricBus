{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import all required Modules and custom build Classes into the Jupyter notebook\n",
    "Import all required modules for the project.<br />\n",
    "Import all custom modules for the project and instantiate classes using the config file.<br />\n",
    "Custom modules can now be called directly in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from sys import getsizeof as dictsize\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pymongo\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "import urllib.request\n",
    "\n",
    "try:\n",
    "    import createNewData.data.config as in_config\n",
    "    from createNewData.pypackages.Azure import Azure\n",
    "    from createNewData.pypackages.urlHandler import UrlHandler\n",
    "    AzurePackage = Azure(in_config)\n",
    "    Url = UrlHandler(in_config)\n",
    "    \n",
    "    \n",
    "except ImportError as e:\n",
    "    print(in_config.FailedImport)\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Upload to SQL Database\n",
    "Upload an entire table from a txt or csv file to the Azure SQL Database.\n",
    "Required fields:\n",
    "* \"UploadToSQL\" is the class name for uploading tables to the database.\n",
    "* \"df\" is the dataframe to be written to the database.\n",
    "* tablename is the name of the new table to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(in_config.shapes)\n",
    "\n",
    "SqlDataCursor = AzurePackage(\"UploadToSQL\",\n",
    "                              df,\n",
    "                              \"shapes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Return All Unique Shape Id's From SQL Database\n",
    "1. Collect all items in the '' schema of shared team Database\n",
    "2. Reduce Dataframe by removing dupicate coordinates.\n",
    "3. Pop() coordinates based on batch sizes no greater than 1024 bytes.\n",
    "4. Add each batch list to a list of batches to send the the Open-elevations API\n",
    "\n",
    "### Attributes\n",
    "* Azure class imported with call functionality (in)\n",
    "* Config File (in)\n",
    "* listOfBatches (out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An unexpected exception occured while attempting to complete this function.\n",
      "('08001', '[08001] [Microsoft][ODBC Driver 13 for SQL Server]Named Pipes Provider: Could not open a connection to SQL Server [53].  (53) (SQLDriverConnect); [08001] [Microsoft][ODBC Driver 13 for SQL Server]Login timeout expired (0); [08001] [Microsoft][ODBC Driver 13 for SQL Server]Invalid connection string attribute (0); [08001] [Microsoft][ODBC Driver 13 for SQL Server]A network-related or instance-specific error has occurred while establishing a connection to SQL Server. Server is not found or not accessible. Check if instance name is correct and if SQL Server is configured to allow remote connections. For more information see SQL Server Books Online. (53)')\n"
     ]
    }
   ],
   "source": [
    "listOfBatches = []\n",
    "batches = {\"locations\" : []}\n",
    "\n",
    "try:\n",
    "    df = AzurePackage(\"SelectAllData\", \"[stops]\")\n",
    "    coordinateDF = df[[\"stop_id\", \"stop_lat\",\"stop_lon\"]]\n",
    "    request = coordinateDF.drop_duplicates(subset=None, \n",
    "                                        keep='first', \n",
    "                                        inplace=False)\n",
    "    coordinatesReq = Url(\"generateLocationRequest\", request)\n",
    "    for key, value in coordinatesReq.items():\n",
    "        if key == \"locations\":\n",
    "            locations = value\n",
    "    while len(locations) != 0:\n",
    "        for each in locations:\n",
    "            if dictsize(batches[\"locations\"]) +\\\n",
    "                                dictsize(each) +\\\n",
    "                                dictsize(batches) < 1024:\n",
    "                location = locations.pop(0)\n",
    "                batches[\"locations\"].append(location)\n",
    "            else:\n",
    "                listOfBatches.append(batches)\n",
    "                batches = {\"locations\" : []}\n",
    "        for each in listOfBatches:\n",
    "            if dictsize(each) > 1024:\n",
    "                raise Exception(in_config.RequestToBig)\n",
    "            else:\n",
    "                pass\n",
    "    else:\n",
    "        print(\"All values added to the list of requests.\")\n",
    "except pd.io.sql.DatabaseError as e:\n",
    "    print(in_config.NoSQLShema)\n",
    "\n",
    "except urllib.request.HTTPError as e:\n",
    "    if e.code == \"403\":\n",
    "        print(in_config.SQLConnectionFail)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(in_config.UNKMGO)\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53.352244</td>\n",
       "      <td>-6.263723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53.352309</td>\n",
       "      <td>-6.263811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53.352575</td>\n",
       "      <td>-6.264175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.352749</td>\n",
       "      <td>-6.264454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53.352841</td>\n",
       "      <td>-6.264570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4184</th>\n",
       "      <td>53.129859</td>\n",
       "      <td>-6.527305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4185</th>\n",
       "      <td>53.129802</td>\n",
       "      <td>-6.527067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4186</th>\n",
       "      <td>53.121040</td>\n",
       "      <td>-6.535928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4187</th>\n",
       "      <td>53.121094</td>\n",
       "      <td>-6.535941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4188</th>\n",
       "      <td>53.116587</td>\n",
       "      <td>-6.538485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4189 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       latitude  longitude\n",
       "0     53.352244  -6.263723\n",
       "1     53.352309  -6.263811\n",
       "2     53.352575  -6.264175\n",
       "3     53.352749  -6.264454\n",
       "4     53.352841  -6.264570\n",
       "...         ...        ...\n",
       "4184  53.129859  -6.527305\n",
       "4185  53.129802  -6.527067\n",
       "4186  53.121040  -6.535928\n",
       "4187  53.121094  -6.535941\n",
       "4188  53.116587  -6.538485\n",
       "\n",
       "[4189 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "for each in listOfBatches:\n",
    "    for x in each[\"locations\"]:\n",
    "        a.append(x)\n",
    "len(a)\n",
    "y = pd.DataFrame(a)\n",
    "y.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Pip module Unable to parse debugpy output, please log an issue with https://github.com/microsoft/vscode-jupyter is required for debugging cells. You will need to install it to debug cells.",
     "output_type": "error",
     "traceback": [
      "Error: Pip module Unable to parse debugpy output, please log an issue with https://github.com/microsoft/vscode-jupyter is required for debugging cells. You will need to install it to debug cells.",
      "at b.parseConnectInfo (c:\\Users\\James\\.vscode\\extensions\\ms-toolsai.jupyter-2021.5.745244803\\out\\client\\extension.js:49:478177)",
      "at b.connectToLocal (c:\\Users\\James\\.vscode\\extensions\\ms-toolsai.jupyter-2021.5.745244803\\out\\client\\extension.js:49:478704)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:97:5)",
      "at async b.connect (c:\\Users\\James\\.vscode\\extensions\\ms-toolsai.jupyter-2021.5.745244803\\out\\client\\extension.js:49:476581)",
      "at async b.startDebugSession (c:\\Users\\James\\.vscode\\extensions\\ms-toolsai.jupyter-2021.5.745244803\\out\\client\\extension.js:49:475728)",
      "at async T.submitCode (c:\\Users\\James\\.vscode\\extensions\\ms-toolsai.jupyter-2021.5.745244803\\out\\client\\extension.js:32:663261)",
      "at async T.handleRunByLine (c:\\Users\\James\\.vscode\\extensions\\ms-toolsai.jupyter-2021.5.745244803\\out\\client\\extension.js:32:651205)"
     ]
    }
   ],
   "source": [
    "listOfObjects = []\n",
    "listOfElevations = []\n",
    "ListOfDicts = []\n",
    "\n",
    "\n",
    "try:\n",
    "    for each in listOfBatches:\n",
    "        attempts = 0\n",
    "        while attempts <5:\n",
    "            try:\n",
    "                ListOfDicts.append(Url(\"mineElevationData\",listOfBatches[0]))\n",
    "            except urllib.error.HTTPError:\n",
    "                attempts = attempts+1\n",
    "            listOfBatches.pop(0)\n",
    "            Iteration = Iteration + 1\n",
    "            print(Iteration)\n",
    "            time.sleep(25)\n",
    "    for each in ListOfDicts:\n",
    "        for key, value in each.items():\n",
    "            if type(value) is list:\n",
    "                listOfObjects.append(value)\n",
    "\n",
    "    for each in listOfObjects:\n",
    "        for elevation in each:\n",
    "            listOfElevations.append(elevation)\n",
    "        \n",
    "\n",
    "    df = pd.DataFrame(listOfElevations)\n",
    "    dfTrimmed = df.drop_duplicates()\n",
    "    sumElevation = dfTrimmed[\"elevation\"].sum()\n",
    "    if type(sumElevation) in [np.int64,int]:\n",
    "        print(\"Elevations collected correctly\")\n",
    "        print(dfTrimmed.head())\n",
    "    else: \n",
    "        raise Exception(\"Failed to collect all elevations, please try again.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Column {e} cannot be found in the dataframe.\")\n",
    "except NameError as e:\n",
    "    print(f\"The Datatable {e} cannot be found.\")\n",
    "except Exception as e:\n",
    "    print(in_config.UNKMGO)\n",
    "    print(type(e))\n",
    "    print(e)\n",
    "\n",
    "\n",
    "#     SqlDataCursor = AzurePackage(\"UploadToSQL\",\n",
    "#                               df,\n",
    "#                               \"elevations\",\n",
    "#                               in_config.connQuote)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfTrimmed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-4ac391041a97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdfTrimmed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dfTrimmed' is not defined"
     ]
    }
   ],
   "source": [
    "dfTrimmed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in shapeData:\n",
    "    for key, value in each.items():\n",
    "        if type(value) is list:\n",
    "            listOfObjects.append(value)\n",
    "\n",
    "    for each in listOfObjects:\n",
    "        for elevation in each:\n",
    "            listOfElevations.append(elevation)\n",
    "\n",
    "    df = pd.DataFrame(listOfElevations)\n",
    "    dfTrimmed = df.drop_duplicates()\n",
    "    sumElevation = dfTrimmed[\"elevation\"].sum()\n",
    "    if type(sumElevation) in [np.int64,int]:\n",
    "        print(\"Elevations collected correctly\")\n",
    "        print(dfTrimmed.head())\n",
    "    else: \n",
    "        raise Exception(\"Failed to collect all elevations, please try again.\")\n",
    "# except KeyError as e:\n",
    "#     print(f\"Column {e} cannot be found in the dataframe.\")\n",
    "# except NameError as e:\n",
    "#     print(f\"The Datatable {e} cannot be found.\")\n",
    "# except Exception as e:\n",
    "#     print(in_config.UNKMGO)\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Elevation Data and Upload to MongoDB\n",
    "Collect the longtitude and lattitude for elevation data from the shapes table.\n",
    "Store data to the mongoDB database in Azure Cosmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shapeList = []\n",
    "AzurePackage(\"DropMongoColl\",\"shapes\")\n",
    "for each in shapeIds.iterrows():\n",
    "    try:\n",
    "        # Generate the Pandas table of all the Longtitudes and Latitudes\n",
    "        # for each shape\n",
    "        elevations = AzurePackage(\"SelectLongLat\",\n",
    "                                \"[shape_id],[shape_pt_lat],[shape_pt_lon]\",\n",
    "                                \"[dbo].[shapes]\",\n",
    "                                \"[shape_id]\",\n",
    "                                each[1][0])\n",
    "\n",
    "        # Generate the Json document for upload to MongoDB\n",
    "        shapeData = Url(\"mineElevationData\",elevations)\n",
    "    except urllib.error.HTTPError as e:\n",
    "        print(in_config.URLOOD)\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        # Upload the Json document to MongoDB\n",
    "        if not len(shapeData) == 0:\n",
    "            AzurePackage(\"UploadToMongo\",\"shapes\",shapeData)\n",
    "        else:\n",
    "            raise Exception(in_config.NDIDF)\n",
    "    except TypeError as e:\n",
    "        print(in_config.TEC)\n",
    "    except pymongo.errors.DuplicateKeyError as e:\n",
    "        print(in_config.FIDB)\n",
    "    except Exception as e:\n",
    "        print(in_config.UNKMGO)\n",
    "        print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data From MongoDB and Write to Pandas DataFrame\n",
    "This reads data from the raw Json files in the MongoDB database and imports them to a dataframe that contains only unique values. \n",
    "Removing any rows where any full row duplicates exist.<br />\n",
    "Finally, this tests that all the elevation data was collected correctly by summing the values of the elevations and therefore ruling out any NaN values. An exception will be raised here in the final script to indicate that the elevation collection was unsuccessfull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "listOfObjects = []\n",
    "listOfElevations = []\n",
    "\n",
    "\n",
    "try:\n",
    "    dbcollections = AzurePackage(\"SelectFromMongo\")\n",
    "    for each in dbcollections.find():\n",
    "        for key, value in each.items():\n",
    "            if type(value) is list:\n",
    "                listOfObjects.append(value)\n",
    "        \n",
    "\n",
    "    for each in listOfObjects:\n",
    "        for elevation in each:\n",
    "            listOfElevations.append(elevation)\n",
    "\n",
    "    df = pd.DataFrame(listOfElevations)\n",
    "    dfTrimmed = df.drop_duplicates()\n",
    "    sumElevation = dfTrimmed[\"elevation\"].sum()\n",
    "    if type(sumElevation) in [np.int64,int]:\n",
    "        print(\"Elevations collected correctly\")\n",
    "        print(dfTrimmed.head())\n",
    "    else: \n",
    "        raise Exception(\"Failed to collect all elevations, please try again.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Column {e} cannot be found in the dataframe.\")\n",
    "except NameError as e:\n",
    "    print(f\"The Datatable {e} cannot be found.\")\n",
    "except Exception as e:\n",
    "    print(in_config.UNKMGO)\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Trimmed Elevations DataFrame to SQL\n",
    "\n",
    "This saves the resulting SQL schemata to the development database.\n",
    "This will overwrite any existing data in the SQL schemata that already exists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SqlDataCursor = AzurePackage(\"UploadToSQL\",\n",
    "                              dfTrimmed,\n",
    "                              \"elevations\",\n",
    "                              in_config.connQuote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect real time data and upload to Mongo\n",
    "This collects the real time data as a json file and overwrites the collection in the mongoDB database.\n",
    "* URL used https://gtfsr.transportforireland.ie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = in_config.url2\n",
    "headers = in_config.RTIheaders\n",
    "response = Url(\"callURL\", url, {}, headers)\n",
    "JsonData = response.read().decode('utf8').replace(\"'\", '\"')\n",
    "RTIgtfs = json.loads(JsonData)\n",
    "try:\n",
    "    AzurePackage(\"DropMongoColl\",\"RTIgtfs\")\n",
    "    AzurePackage(\"UploadToMongo\",\"RTIgtfs\",RTIgtfs)\n",
    "except pymongo.errors.WriteError as e:\n",
    "    print(\"An error occured while attempting to write the GTFS data to Mongo Database.\")\n",
    "    print(type(e))\n",
    "except HTTPError as e:\n",
    "    print(\"An error occured while attempting to connect to the Mongo Database.\")\n",
    "    print(type(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Trimmed Elevation Data Team SQL Database\n",
    "This reads data from the raw Json files in the MongoDB database and imports them to a dataframe that contains only unique values. Removing any rows where any full row duplicates exist.\n",
    "This saves the resulting SQL schemata to the Database used by the R-Shiny app. (Production)\n",
    "This will overwrite any existing data in the SQL schemata that already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SqlDataCursor = AzurePackage(\"UploadToSQL\",\n",
    "                              dfTrimmed,\n",
    "                              \"elevations\",\n",
    "                              in_config.teamConnQuote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect the shape and elevation schema together by joining their longtitude and latatude values\n",
    "This will either be a method in the Rshiny app to collect or we simply create a new database from this data but this seems a bit verbose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = AzurePackage(\"AzureDBConn\", in_config.connQuote)\n",
    "SQLString = in_config.SQLElevation\n",
    "df = pd.read_sql(SQLString, conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = AzurePackage(\"AzureDBConn\", in_config.teamConnQuote)\n",
    "SQLString = \"\"\"Select * FROM [distances]\"\"\"\n",
    "df = pd.read_sql(SQLString, conn)\n",
    "print(df.head())\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        route_id service_id  quasi_block          stop   stop_lat  stop_lon  \\\n",
      "0    60-16-d12-1      y1004          1.0  8230DB001325  53.286533 -6.282408   \n",
      "1    60-16-d12-1      y1004          1.0  8230DB001326  53.288805 -6.282902   \n",
      "2    60-16-d12-1      y1004          1.0  8230DB001327  53.291191 -6.282536   \n",
      "3    60-16-d12-1      y1004          1.0  8230DB001328  53.292204 -6.282360   \n",
      "4    60-16-d12-1      y1004          1.0  8230DB001329  53.294967 -6.281994   \n",
      "..           ...        ...          ...           ...        ...       ...   \n",
      "995  60-16-d12-1      y1004          2.0  8230DB002991  53.283317 -6.279387   \n",
      "996  60-16-d12-1      y1004          2.0  8230DB002992  53.284628 -6.281194   \n",
      "997  60-16-d12-1      y1004          2.0  8230DB001325  53.286533 -6.282408   \n",
      "998  60-16-d12-1      y1004          2.0  8230DB001326  53.288805 -6.282902   \n",
      "999  60-16-d12-1      y1004          2.0  8230DB001327  53.291191 -6.282536   \n",
      "\n",
      "    elevation  \n",
      "0        None  \n",
      "1        None  \n",
      "2        None  \n",
      "3        None  \n",
      "4        None  \n",
      "..        ...  \n",
      "995      None  \n",
      "996      None  \n",
      "997      None  \n",
      "998      None  \n",
      "999      None  \n",
      "\n",
      "[1000 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "conn = AzurePackage(\"AzureDBConn\", in_config.teamConnQuote)\n",
    "SQLString = \"\"\"Select TOP 1000 distances.route_id,distances.service_id,distances.quasi_block,distances.stop, \n",
    "               stops.stop_lat,stops.stop_lon,\n",
    "               elevations.elevation\n",
    "               FROM distances\n",
    "               LEFT JOIN stops ON distances.stop = stops.stop_id\n",
    "               LEFT JOIN elevations ON stops.stop_lat = elevations.latitude AND stops.stop_lon = elevations.longitude\n",
    "               \"\"\"\n",
    "df = pd.read_sql(SQLString, conn)\n",
    "print(df)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = AzurePackage(\"AzureDBConn\", in_config.teamConnQuote)\n",
    "SQLString = \"\"\"Select stops.stop_lat,stops.stop_lon,\n",
    "               elevations.elevation\n",
    "               FROM stops\n",
    "               LEFT JOIN elevations ON stops.stop_lat = elevations.latitude AND stops.stop_lon = elevations.longitude\n",
    "               \"\"\"\n",
    "df = pd.read_sql(SQLString, conn)\n",
    "print(df)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.isna().any(axis=1)]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'locations': [{'longtitude': 123, 'lattitude': 45}, {'...'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"locations\": [{\"longtitude\" : 123, \"lattitude\" : 45}, {\"...\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "from kafka.errors import KafkaError\n",
    "from datetime import datetime\n",
    "import psutil\n",
    "import time\n",
    "import json\n",
    "\n",
    "kafka_broker='dap:9092' \n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers = [kafka_broker],\n",
    "api_version=(0,11,5),\n",
    "    value_serializer = lambda m: json.dumps(m).encode()\n",
    ")\n",
    "\n",
    "\n",
    "while True :\n",
    "    cpu_times = psutil.cpu_times()._asdict()\n",
    "    cpu_times['datetime'] = datetime.now().strftime(\"%d/%m/%Y, %H:%M:%S\")\n",
    "    future = producer.send('cpu',cpu_times)\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "kafka_broker='dap:9092' \n",
    "\n",
    "consumer = KafkaConsumer('cpu',\n",
    "                         bootstrap_servers = [kafka_broker],\n",
    "                         auto_offset_reset = 'earliest',\n",
    "                         group_id = None,\n",
    "                         consumer_timeout_ms = 10000)\n",
    "for message in consumer:\n",
    "    print(message.value.decode())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
