{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 1. Import all required Modules and custom build Classes into the Jupyter notebook\n",
    "Import all required modules for the project.<br />\n",
    "Import all custom modules for the project and instantiate classes using the config file.<br />\n",
    "Custom modules can now be called directly in the notebook.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from sys import getsizeof as dictsize\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pymongo\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "import urllib.request\n",
    "import missingno as msno\n",
    "import zipfile \n",
    "import io\n",
    "\n",
    "try:\n",
    "    import createNewData.data.config as in_config\n",
    "    from createNewData.pypackages.Azure import Azure\n",
    "    from createNewData.pypackages.urlHandler import UrlHandler\n",
    "    AzurePackage = Azure(in_config)\n",
    "    Url = UrlHandler(in_config)\n",
    "    \n",
    "    \n",
    "except ImportError as e:\n",
    "    print(in_config.FailedImport)\n",
    "    print(e)"
   ]
  },
  {
   "source": [
    "# 2. Upload to SQL Database\n",
    "Download the entire GTFS data and upload to the Azure SQL Database.\n",
    "Required fields:\n",
    "* \"UploadToSQL\": The class name for the Azure constructor for uploading tables to the database.\n",
    "* GTFSDF:  The dataframes to be written to the database.\n",
    "* tablename: The name of the new table to be created.\n",
    "* teamConnQuote: The connection string for the team SQL Database obtained from the config file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GTFS = requests.get(in_config.GTFSURL, stream=True)\n",
    "GTFSRaw = zipfile.ZipFile(io.BytesIO(GTFS.content))\n",
    "\n",
    "for each in GTFSRaw.namelist():\n",
    "    GTFSDF = pd.read_csv(GTFSRaw.open(each), low_memory=False)\n",
    "    tableName = each.replace(\".txt\",\"\")\n",
    "    AzurePackage(\"UploadToSQL\",\n",
    "                  GTFSDF,\n",
    "                  tableName,\n",
    "                  in_config.teamConnQuote)"
   ]
  },
  {
   "source": [
    "# 3. Return All Unique Stop Id's From SQL Database\n",
    "1. Collect all items in the '' schema of shared team Database\n",
    "2. Reduce Dataframe by removing dupicate coordinates.\n",
    "3. Pop() coordinates based on batch sizes no greater than 1024 bytes.\n",
    "4. Add each batch list to a list of batches to send the the Open-elevations API\n",
    "\n",
    "### Attributes\n",
    "* Azure class imported with call functionality (in)\n",
    "* Config File (in)\n",
    "* listOfBatches (out)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "listOfBatches = []\n",
    "batches = {\"locations\" : []}\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\"ingestRawData\\gtfs\\shapes.txt\")\n",
    "    shapesDF = df[[\"shape_id\", \"shape_pt_lat\",\"shape_pt_lon\"]]\n",
    "    shapesRequest = shapesDF.drop_duplicates(subset=None, \n",
    "                                        keep='first', \n",
    "                                        inplace=False)\n",
    "    shapesCoordinates = Url(\"generateLocationRequest\", shapesRequest)\n",
    "    for key, value in shapesCoordinates.items():\n",
    "        if key == \"locations\":\n",
    "            locations = value\n",
    "    while len(locations) != 0:\n",
    "        for each in locations:\n",
    "            if dictsize(batches[\"locations\"]) +\\\n",
    "                                dictsize(each) +\\\n",
    "                                dictsize(batches) < 9700:\n",
    "                location = locations.pop(0)\n",
    "                batches[\"locations\"].append(location)\n",
    "            else:\n",
    "                location = locations.pop(0)\n",
    "                batches[\"locations\"].append(location)\n",
    "                listOfBatches.append(batches)\n",
    "                batches = {\"locations\" : []}\n",
    "        for each in listOfBatches:\n",
    "            if dictsize(each) > 10000:\n",
    "                raise Exception(in_config.RequestToBig)\n",
    "            else:\n",
    "                pass\n",
    "    else:\n",
    "        print(\"All values added to the list of requests.\")\n",
    "except pd.io.sql.DatabaseError as e:\n",
    "    print(in_config.NoSQLShema)\n",
    "\n",
    "except urllib.request.HTTPError as e:\n",
    "    if e.code == \"403\":\n",
    "        print(in_config.SQLConnectionFail)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(in_config.UNKMGO)\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Request Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfObjects = []\n",
    "listOfElevations = []\n",
    "ListOfDicts = []\n",
    "\n",
    "Iteration = 0\n",
    "try:\n",
    "    for each in listOfBatches:\n",
    "            attempts = 0\n",
    "            while attempts < 5:\n",
    "                try:\n",
    "                    ListOfDicts.append(Url(\"mineElevationData\",each))\n",
    "                    break\n",
    "                except urllib.error.HTTPError:\n",
    "                    attempts = attempts+1\n",
    "            Iteration = Iteration + 1\n",
    "            print(Iteration)\n",
    "            time.sleep(2)\n",
    "    for each in ListOfDicts:\n",
    "        for key, value in each.items():\n",
    "            if type(value) is list:\n",
    "                listOfObjects.append(value)\n",
    "\n",
    "    for each in listOfObjects:\n",
    "        for elevation in each:\n",
    "            listOfElevations.append(elevation)\n",
    "        \n",
    "\n",
    "    df = pd.DataFrame(listOfElevations)\n",
    "    dfTrimmed = df.drop_duplicates()\n",
    "    sumElevation = dfTrimmed[\"elevation\"].sum()\n",
    "    if type(sumElevation) in [np.int64,int]:\n",
    "        print(\"Elevations collected correctly\")\n",
    "        print(dfTrimmed.head())\n",
    "    else: \n",
    "        raise Exception(\"Failed to collect all elevations, please try again.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Column {e} cannot be found in the dataframe.\")\n",
    "except NameError as e:\n",
    "    print(f\"The Datatable {e} cannot be found.\")\n",
    "except Exception as e:\n",
    "    print(in_config.UNKMGO)\n",
    "    print(type(e))\n",
    "    print(e)\n",
    "\n",
    "    "
   ]
  },
  {
   "source": [
    "# Save Trimmed Elevation Data Team SQL Database\n",
    "This reads data from the raw Json files in the MongoDB database and imports them to a dataframe that contains only unique values. Removing any rows where any full row duplicates exist.\n",
    "This saves the resulting SQL schemata to the Database used by the R-Shiny app. (Production)\n",
    "This will overwrite any existing data in the SQL schemata that already exists."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SqlDataCursor = AzurePackage(\"UploadToSQL\",\n",
    "                              dfTrimmed,\n",
    "                              \"elevations\",\n",
    "                              in_config.teamConnQuote)"
   ]
  },
  {
   "source": [
    "# Collect Elevation Data and Upload to MongoDB\n",
    "Collect the longtitude and lattitude for elevation data from the shapes table.\n",
    "Store data to the mongoDB database in Azure Cosmos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shapeList = []\n",
    "AzurePackage(\"DropMongoColl\",\"shapes\")\n",
    "for each in shapeIds.iterrows():\n",
    "    try:\n",
    "        # Generate the Pandas table of all the Longtitudes and Latitudes\n",
    "        # for each shape\n",
    "        elevations = AzurePackage(\"SelectLongLat\",\n",
    "                                \"[shape_id],[shape_pt_lat],[shape_pt_lon]\",\n",
    "                                \"[dbo].[shapes]\",\n",
    "                                \"[shape_id]\",\n",
    "                                each[1][0])\n",
    "\n",
    "        # Generate the Json document for upload to MongoDB\n",
    "        shapeData = Url(\"mineElevationData\",elevations)\n",
    "    except urllib.error.HTTPError as e:\n",
    "        print(in_config.URLOOD)\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        # Upload the Json document to MongoDB\n",
    "        if not len(shapeData) == 0:\n",
    "            AzurePackage(\"UploadToMongo\",\"shapes\",shapeData)\n",
    "        else:\n",
    "            raise Exception(in_config.NDIDF)\n",
    "    except TypeError as e:\n",
    "        print(in_config.TEC)\n",
    "    except pymongo.errors.DuplicateKeyError as e:\n",
    "        print(in_config.FIDB)\n",
    "    except Exception as e:\n",
    "        print(in_config.UNKMGO)\n",
    "        print(e)\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Read Data From MongoDB and Write to Pandas DataFrame\n",
    "This reads data from the raw Json files in the MongoDB database and imports them to a dataframe that contains only unique values. \n",
    "Removing any rows where any full row duplicates exist.<br />\n",
    "Finally, this tests that all the elevation data was collected correctly by summing the values of the elevations and therefore ruling out any NaN values. An exception will be raised here in the final script to indicate that the elevation collection was unsuccessfull."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Upload Trimmed Elevations DataFrame to SQL\n",
    "\n",
    "This saves the resulting SQL schemata to the development database.\n",
    "This will overwrite any existing data in the SQL schemata that already exists.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrimmed = pd.read_csv(r\"C:\\Users\\James\\Documents\\MSc in Data Analytics\\Database and Ananytics\\Research Project\\dapTbElectricDublinBus\\Elevations.csv\")\n",
    "SqlDataCursor = AzurePackage(\"UploadToSQL\",\n",
    "                              dfTrimmed,\n",
    "                              \"elevations\",\n",
    "                              in_config.teamConnQuote)"
   ]
  },
  {
   "source": [
    "# Collect real time data and upload to Mongo\n",
    "This collects the real time data as a json file and overwrites the collection in the mongoDB database.\n",
    "* URL used https://gtfsr.transportforireland.ie\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = in_config.url2\n",
    "headers = in_config.RTIheaders\n",
    "response = Url(\"callURL\", url, {}, headers)\n",
    "JsonData = response.read().decode('utf8').replace(\"'\", '\"')\n",
    "RTIgtfs = json.loads(JsonData)\n",
    "try:\n",
    "    AzurePackage(\"UploadToMongo\",\"RTIgtfs\",RTIgtfs)\n",
    "except pymongo.errors.WriteError as e:\n",
    "    print(\"An error occured while attempting to write the GTFS data to Mongo Database.\")\n",
    "    print(type(e))\n",
    "except HTTPError as e:\n",
    "    print(\"An error occured while attempting to connect to the CosmosDB Database.\")\n",
    "    print(type(e))\n"
   ]
  },
  {
   "source": [
    "# Connect the shape and elevation schema together by joining their longtitude and latatude values\n",
    "This will either be a method in the Rshiny app to collect or we simply create a new database from this data but this seems a bit verbose."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = AzurePackage(\"AzureDBConn\", in_config.teamConnQuote)\n",
    "shapeListDF = []\n",
    "\n",
    "SQLTrips = \"\"\"SELECT distances.route_id, \n",
    "                     distances.service_id, \n",
    "                     distances.quasi_block,\n",
    "                     distances.trip_id,\n",
    "                     trips.shape_id\n",
    "               FROM distances\n",
    "               JOIN trips \n",
    "               ON distances.trip_id = trips.trip_id\n",
    "               WHERE distances.route_id = '60-1-d12-1'\n",
    "               AND distances.service_id = 'y1002'\n",
    "               AND distances.quasi_block = '3.0'\n",
    "               GROUP BY distances.route_id, distances.service_id, distances.quasi_block, distances.trip_id, trips.shape_id\n",
    "               \"\"\"\n",
    "tripsIds = pd.read_sql(SQLTrips, conn)\n",
    "\n",
    "\n",
    "for each in tripsIds[\"shape_id\"]:\n",
    "    SQLShapes = f\"\"\"SELECT shapes.shape_id, shapes.shape_pt_sequence, elevations.elevation\n",
    "                    FROM shapes\n",
    "                    LEFT JOIN elevations ON\n",
    "                    shapes.shape_pt_lon = elevations.longitude \n",
    "                    AND shapes.shape_pt_lat = elevations.latitude\n",
    "                    WHERE shapes.shape_id = '{each}'\n",
    "                    ORDER BY shapes.shape_id, shapes.shape_pt_sequence\n",
    "                    \"\"\"\n",
    "                    \n",
    "    shapeIDs = pd.read_sql(SQLShapes, conn)\n",
    "    shapeListDF.append(shapeIDs)\n",
    "fullShapesDF = pd.concat(shapeListDF).reset_index(drop=True)\n",
    "\n",
    "print(fullShapesDF)\n",
    "# fullShapesDF.to_csv(\"shapeIDs.csv\")\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in tripsIds[\"shape_id\"]:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.rv(\"rawDistace.csv\")\n",
    "dfstop = pd.read_csv(r\"\"\"C:\\Users\\James\\Documents\\MSc in Data Analytics\\Database and Ananytics\\Research Project\\dapTbElectricDublinBus\\ingestRawData\\gtfs\\stops.txt\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = AzurePackage(\"AzureDBConn\", in_config.teamConnQuote)\n",
    "SQLString = \"\"\"Select top 1000 * FROM [distances]\"\"\"\n",
    "stopdf = pd.read_sql(SQLString, conn)\n",
    "print(stopdf.head(5))\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = elevationdf[[\"stop\"]].drop_duplicates()\n",
    "distances = distancedf[[\"stop\"]].drop_duplicates()\n",
    "\n",
    "if len(distances) == len(merged):\n",
    "    print(\"No elevations missing from the merged data.\")\n",
    "else:\n",
    "    mergedTest = pd.merge(distances, merged, how='outer', indicator='Exist')\n",
    "\n",
    "mergedTest = mergedTest.replace(\"left_only\", np.nan)\n",
    "msno.matrix(mergedTest)\n",
    "# mergedTest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "\n",
    "GTFS = requests.get(in_config.GTFSURL, stream=True)\n",
    "GTFSRaw = zipfile.ZipFile(io.BytesIO(GTFS.content))\n",
    "\n",
    "for each in GTFSRaw.namelist():\n",
    "    GTFSDF = pd.read_csv(GTFSRaw.open(each), low_memory=False)\n",
    "    print(GTFSDF.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}