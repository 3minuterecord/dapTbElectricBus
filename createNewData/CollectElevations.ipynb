{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Import all required Modules and custom build Classes into the Jupyter notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pymongo\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "import urllib.request\n",
    "\n",
    "try:\n",
    "    import createNewData.data.config as in_config\n",
    "    from createNewData.pypackages.Azure import Azure\n",
    "    from createNewData.pypackages.urlHandler import UrlHandler\n",
    "    AzurePackage = Azure(in_config)\n",
    "    Url = UrlHandler(in_config)\n",
    "    \n",
    "    \n",
    "except ImportError as e:\n",
    "    print(\"Failed to import critical modules for this script.\")\n",
    "    print(\"Please confirm that files exist in the correct locations.\")\n",
    "    print(e)"
   ]
  },
  {
   "source": [
    "# Upload to SQL Database\n",
    "The below can upload an entire table from a txt or csv file to the Azure SQL Database.\n",
    "Required fields:\n",
    "* \"UploadToSQL\" is the class name for uploading tables to the database.\n",
    "* tablepath is the path the file is located in.\n",
    "* tablename is the name of the new table to be created."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\James\\\\Documents\\\\MSc in Data Analytics\\\\Database and Ananytics\\\\Research Project\\\\dapTbElectricDublinBus\\\\ingestRawData\\\\raw\\\\shapes.txt'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-29f7235c3cd8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m SqlDataCursor = AzurePackage(\"UploadToSQL\",\n\u001b[0;32m      4\u001b[0m                               \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                               \"shapes\")\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\James\\\\Documents\\\\MSc in Data Analytics\\\\Database and Ananytics\\\\Research Project\\\\dapTbElectricDublinBus\\\\ingestRawData\\\\raw\\\\shapes.txt'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(in_config.shapes)\n",
    "\n",
    "SqlDataCursor = AzurePackage(\"UploadToSQL\",\n",
    "                              df,\n",
    "                              \"shapes\")"
   ]
  },
  {
   "source": [
    "# Return All Unique Shape Id's From SQL Database\n",
    "The below collects all unique values in a specific column of a database.\n",
    "Required fields:\n",
    "* \"SelectDistinct\" is the class name for collecting the unique values.\n",
    "* shape_id is the column name.\n",
    "* [[dbo].[shapes]] is the name of the table to be queried."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapeIds = AzurePackage(\"SelectDistinct\",\n",
    "                              \"shape_id\",\n",
    "                              \"[dbo].[shapes]\")"
   ]
  },
  {
   "source": [
    "# Collect Elevation Data and Upload to MongoDB\n",
    "The below collects the longtitude and lattitude for elevation data from the shapes table.\n",
    "It then generates and send the request to the open elevations URL.\n",
    "Finally it stores this data to the mongoDB database in Azure Cosmos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shapeList = []\n",
    "AzurePackage(\"DropMongoColl\",\"shapes\")\n",
    "for each in shapeIds.iterrows():\n",
    "    try:\n",
    "        # Generate the Pandas table of all the Longtitudes and Latitudes\n",
    "        # for each shape\n",
    "        elevations = AzurePackage(\"SelectLongLat\",\n",
    "                                \"[shape_id],[shape_pt_lat],[shape_pt_lon]\",\n",
    "                                \"[dbo].[shapes]\",\n",
    "                                \"[shape_id]\",\n",
    "                                each[1][0])\n",
    "\n",
    "        # Generate the Json document for upload to MongoDB\n",
    "        shapeData = Url(\"mineElevationData\",elevations)\n",
    "    except urllib.error.HTTPError as e:\n",
    "        print(in_config.URLOOD)\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        # Upload the Json document to MongoDB\n",
    "        if not len(shapeData) == 0:\n",
    "            AzurePackage(\"UploadToMongo\",\"shapes\",shapeData)\n",
    "        else:\n",
    "            raise Exception(in_config.NDIDF)\n",
    "    except TypeError as e:\n",
    "        print(in_config.TEC)\n",
    "    except pymongo.errors.DuplicateKeyError as e:\n",
    "        print(in_config.FIDB)\n",
    "    except Exception as e:\n",
    "        print(in_config.UNKMGO)\n",
    "        print(e)\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Read Data From MongoDB and Write to Pandas DataFrame\n",
    "This reads data from the raw Json files in the MongoDB database and imports them to a dataframe that contains only unique values. \n",
    "Removing any rows where any full row duplicates exist.<br />\n",
    "Finally, this tests that all the elevation data was collected correctly by summing the values of the elevations and therefore ruling out any NaN values. An exception will be raised here in the final script to indicate that the elevation collection was unsuccessfull."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Elevations collected correctly\n    latitude  elevation  longitude\n0  53.391176         55  -6.262199\n1  53.391189         55  -6.262439\n2  53.391871         59  -6.262366\n3  53.391854         59  -6.260548\n4  53.391815         58  -6.259722\n"
     ]
    }
   ],
   "source": [
    "listOfObjects = []\n",
    "listOfElevations = []\n",
    "\n",
    "\n",
    "try:\n",
    "    dbcollections = AzurePackage(\"SelectFromMongo\")\n",
    "    for each in dbcollections.find():\n",
    "        for key, value in each.items():\n",
    "            if type(value) is list:\n",
    "                listOfObjects.append(value)\n",
    "        \n",
    "\n",
    "    for each in listOfObjects:\n",
    "        for elevation in each:\n",
    "            listOfElevations.append(elevation)\n",
    "\n",
    "    df = pd.DataFrame(listOfElevations)\n",
    "    dfTrimmed = df.drop_duplicates()\n",
    "    sumElevation = dfTrimmed[\"elevation\"].sum()\n",
    "    if type(sumElevation) in [np.int64,int]:\n",
    "        print(\"Elevations collected correctly\")\n",
    "        print(dfTrimmed.head())\n",
    "    else: \n",
    "        raise Exception(\"Failed to collect all elevations, please try again.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Column {e} cannot be found in the dataframe.\")\n",
    "except NameError as e:\n",
    "    print(f\"The Datatable {e} cannot be found.\")\n",
    "except Exception as e:\n",
    "    print(in_config.UNKMGO)\n",
    "    print(e)"
   ]
  },
  {
   "source": [
    "# Upload Trimmed Elevations DataFrame to SQL\n",
    "\n",
    "This saves the resulting SQL schemata to the development database.\n",
    "This will overwrite any existing data in the SQL schemata that already exists.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SqlDataCursor = AzurePackage(\"UploadToSQL\",\n",
    "                              dfTrimmed,\n",
    "                              \"elevations\",\n",
    "                              in_config.connQuote)"
   ]
  },
  {
   "source": [
    "# Collect real time data and upload to Mongo\n",
    "This collects the real time data as a json file and overwrites the collection in the mongoDB database.\n",
    "* URL used https://gtfsr.transportforireland.ie\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "An error occured while attempting to write the GTFS data to Mongo DB.\n<class 'pymongo.errors.WriteError'>\n"
     ]
    }
   ],
   "source": [
    "url = in_config.url2\n",
    "headers = in_config.RTIheaders\n",
    "response = Url(\"callURL\", url, {}, headers)\n",
    "JsonData = response.read().decode('utf8').replace(\"'\", '\"')\n",
    "RTIgtfs = json.loads(JsonData)\n",
    "try:\n",
    "    AzurePackage(\"DropMongoColl\",\"RTIgtfs\")\n",
    "    AzurePackage(\"UploadToMongo\",\"RTIgtfs\",RTIgtfs)\n",
    "except pymongo.errors.WriteError as e:\n",
    "    print(\"An error occured while attempting to write the GTFS data to Mongo DB.\")\n",
    "    print(type(e))\n"
   ]
  },
  {
   "source": [
    "# Save Trimmed Elevation Data Team SQL Database\n",
    "This reads data from the raw Json files in the MongoDB database and imports them to a dataframe that contains only unique values. Removing any rows where any full row duplicates exist.\n",
    "This saves the resulting SQL schemata to the Database used by the R-Shiny app. (Production)\n",
    "This will overwrite any existing data in the SQL schemata that already exists."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SqlDataCursor = AzurePackage(\"UploadToSQL\",\n",
    "                              dfTrimmed,\n",
    "                              \"elevations\",\n",
    "                              in_config.teamConnQuote)"
   ]
  },
  {
   "source": [
    "# Connect the shape and elevation schema together by joining their longtitude and latatude values\n",
    "This will either be a method in the Rshiny app to collect or we simply create a new database from this data but this seems a bit verbose."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = AzurePackage(\"AzureDBConn\", in_config.connQuote)\n",
    "SQLString = in_config.SQLElevation\n",
    "df = pd.read_sql(SQLString, conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "type(dfTrimmed[\"elevation\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(19876, )]\n"
     ]
    }
   ],
   "source": [
    "conn = AzurePackage(\"AzureDBConn\", in_config.teamConnQuote)\n",
    "cursor = conn.cursor()\n",
    "a = cursor.execute(\"SELECT COUNT(*) FROM [dbo].[elevations]\")\n",
    "print(a.fetchall())\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            shape_id\n",
       "0     60-1-b12-1.1.O\n",
       "1     60-1-b12-1.2.O\n",
       "2     60-1-b12-1.3.I\n",
       "3     60-1-b12-1.4.I\n",
       "4     60-1-d12-1.1.O\n",
       "..               ...\n",
       "652  60-9-b12-1.13.I\n",
       "653  60-9-d12-1.10.O\n",
       "654  60-9-d12-1.11.O\n",
       "655  60-9-d12-1.12.I\n",
       "656  60-9-d12-1.13.I\n",
       "\n",
       "[657 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>shape_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>60-1-b12-1.1.O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>60-1-b12-1.2.O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>60-1-b12-1.3.I</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>60-1-b12-1.4.I</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>60-1-d12-1.1.O</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>652</th>\n      <td>60-9-b12-1.13.I</td>\n    </tr>\n    <tr>\n      <th>653</th>\n      <td>60-9-d12-1.10.O</td>\n    </tr>\n    <tr>\n      <th>654</th>\n      <td>60-9-d12-1.11.O</td>\n    </tr>\n    <tr>\n      <th>655</th>\n      <td>60-9-d12-1.12.I</td>\n    </tr>\n    <tr>\n      <th>656</th>\n      <td>60-9-d12-1.13.I</td>\n    </tr>\n  </tbody>\n</table>\n<p>657 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "shapeIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}